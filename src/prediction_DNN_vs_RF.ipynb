{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type - Prediction\n",
    "\n",
    "Hi! Thanks for checking this notebook. We'll we working on the Forest Cover Type dataset, which contains tree observations from four areas of the Roosevelt National Forest in Colorado.\n",
    "\n",
    "In this notebook we'll try to predict the forest cover type given the cartographic variables the dataset provides by comparing DNN's to a random forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data and format the target so it ranges from 0 to 6 rather than from 1 to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/covtype.csv')\n",
    "\n",
    "data.Cover_Type = data.Cover_Type - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the train-test split with stratify, so the subsets have the same proportion of classes as the original dataset. Also there is no need to drop one category since we are not dealing with linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dt = data.copy(deep=True)\n",
    "\n",
    "X = dt[[col for col in data.columns if col != 'Cover_Type']]\n",
    "y = dt[['Cover_Type']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, \n",
    "                                                    random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to scale the numerical features since they all have different scales. Since there aren't too many or too big in magnitude outliers, standard scaling should work fine. Note that the scaling is done considering only the train data since we can't leak information of the test data to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = X_train.iloc[:,:10]\n",
    "X_test_num = X_test.iloc[:,:10]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train_num)\n",
    "\n",
    "X_train_num = pd.DataFrame(std_scaler.transform(X_train_num), \n",
    "                        columns=X_train_num.columns,\n",
    "                        index=X_train.index)\n",
    "\n",
    "X_train = pd.concat([X_train_num, X_train.iloc[:,10:]], axis=1)\n",
    "\n",
    "X_test_num = pd.DataFrame(std_scaler.transform(X_test_num), \n",
    "                        columns=X_test_num.columns,\n",
    "                        index=X_test_num.index)\n",
    "\n",
    "X_test = pd.concat([X_test_num, X_test.iloc[:,10:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll try the Deep Neural Net with dropout for regularization. We define the model's arquitecture, compile and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1], )),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', 'categorical_accuracy', 'sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 435759 samples, validate on 145253 samples\n",
      "Epoch 1/25\n",
      "435759/435759 [==============================] - 28s 63us/sample - loss: 0.5844 - accuracy: 0.7520 - categorical_accuracy: 0.3356 - sparse_categorical_accuracy: 0.7520 - val_loss: 0.4769 - val_accuracy: 0.7949 - val_categorical_accuracy: 0.3349 - val_sparse_categorical_accuracy: 0.7949\n",
      "Epoch 2/25\n",
      "435759/435759 [==============================] - 23s 54us/sample - loss: 0.4858 - accuracy: 0.7924 - categorical_accuracy: 0.3307 - sparse_categorical_accuracy: 0.7924 - val_loss: 0.4275 - val_accuracy: 0.8217 - val_categorical_accuracy: 0.3305 - val_sparse_categorical_accuracy: 0.8217\n",
      "Epoch 3/25\n",
      "435759/435759 [==============================] - 20s 46us/sample - loss: 0.4520 - accuracy: 0.8086 - categorical_accuracy: 0.3341 - sparse_categorical_accuracy: 0.8086 - val_loss: 0.4094 - val_accuracy: 0.8315 - val_categorical_accuracy: 0.3705 - val_sparse_categorical_accuracy: 0.8315\n",
      "Epoch 4/25\n",
      "435759/435759 [==============================] - 20s 45us/sample - loss: 0.4331 - accuracy: 0.8175 - categorical_accuracy: 0.3355 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.3882 - val_accuracy: 0.8415 - val_categorical_accuracy: 0.3316 - val_sparse_categorical_accuracy: 0.8415\n",
      "Epoch 5/25\n",
      "435759/435759 [==============================] - 20s 47us/sample - loss: 0.4183 - accuracy: 0.8248 - categorical_accuracy: 0.3372 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.3730 - val_accuracy: 0.8466 - val_categorical_accuracy: 0.3294 - val_sparse_categorical_accuracy: 0.8466\n",
      "Epoch 6/25\n",
      "435759/435759 [==============================] - 21s 47us/sample - loss: 0.4074 - accuracy: 0.8296 - categorical_accuracy: 0.3382 - sparse_categorical_accuracy: 0.8296 - val_loss: 0.3675 - val_accuracy: 0.8467 - val_categorical_accuracy: 0.3580 - val_sparse_categorical_accuracy: 0.8467\n",
      "Epoch 7/25\n",
      "435759/435759 [==============================] - 29s 65us/sample - loss: 0.4006 - accuracy: 0.8324 - categorical_accuracy: 0.3384 - sparse_categorical_accuracy: 0.8324 - val_loss: 0.3599 - val_accuracy: 0.8532 - val_categorical_accuracy: 0.3399 - val_sparse_categorical_accuracy: 0.8532\n",
      "Epoch 8/25\n",
      "435759/435759 [==============================] - 26s 59us/sample - loss: 0.3944 - accuracy: 0.8353 - categorical_accuracy: 0.3400 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.3529 - val_accuracy: 0.8567 - val_categorical_accuracy: 0.3476 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 9/25\n",
      "435759/435759 [==============================] - 38s 86us/sample - loss: 0.3879 - accuracy: 0.8378 - categorical_accuracy: 0.3411 - sparse_categorical_accuracy: 0.8378 - val_loss: 0.3474 - val_accuracy: 0.8581 - val_categorical_accuracy: 0.3452 - val_sparse_categorical_accuracy: 0.8581\n",
      "Epoch 10/25\n",
      "435759/435759 [==============================] - 28s 65us/sample - loss: 0.3830 - accuracy: 0.8402 - categorical_accuracy: 0.3403 - sparse_categorical_accuracy: 0.8402 - val_loss: 0.3429 - val_accuracy: 0.8599 - val_categorical_accuracy: 0.3399 - val_sparse_categorical_accuracy: 0.8599\n",
      "Epoch 11/25\n",
      "435759/435759 [==============================] - 26s 59us/sample - loss: 0.3785 - accuracy: 0.8426 - categorical_accuracy: 0.3419 - sparse_categorical_accuracy: 0.8426 - val_loss: 0.3405 - val_accuracy: 0.8623 - val_categorical_accuracy: 0.3465 - val_sparse_categorical_accuracy: 0.8623\n",
      "Epoch 12/25\n",
      "435759/435759 [==============================] - 26s 60us/sample - loss: 0.3739 - accuracy: 0.8447 - categorical_accuracy: 0.3420 - sparse_categorical_accuracy: 0.8447 - val_loss: 0.3399 - val_accuracy: 0.8615 - val_categorical_accuracy: 0.3260 - val_sparse_categorical_accuracy: 0.8615\n",
      "Epoch 13/25\n",
      "435759/435759 [==============================] - 25s 57us/sample - loss: 0.3717 - accuracy: 0.8454 - categorical_accuracy: 0.3419 - sparse_categorical_accuracy: 0.8454 - val_loss: 0.3326 - val_accuracy: 0.8639 - val_categorical_accuracy: 0.3430 - val_sparse_categorical_accuracy: 0.8639\n",
      "Epoch 14/25\n",
      "435759/435759 [==============================] - 24s 54us/sample - loss: 0.3688 - accuracy: 0.8469 - categorical_accuracy: 0.3437 - sparse_categorical_accuracy: 0.8469 - val_loss: 0.3284 - val_accuracy: 0.8693 - val_categorical_accuracy: 0.3402 - val_sparse_categorical_accuracy: 0.8693\n",
      "Epoch 15/25\n",
      "435759/435759 [==============================] - 21s 48us/sample - loss: 0.3662 - accuracy: 0.8484 - categorical_accuracy: 0.3438 - sparse_categorical_accuracy: 0.8484 - val_loss: 0.3218 - val_accuracy: 0.8712 - val_categorical_accuracy: 0.3450 - val_sparse_categorical_accuracy: 0.8712\n",
      "Epoch 16/25\n",
      "435759/435759 [==============================] - 21s 48us/sample - loss: 0.3633 - accuracy: 0.8489 - categorical_accuracy: 0.3443 - sparse_categorical_accuracy: 0.8489 - val_loss: 0.3273 - val_accuracy: 0.8671 - val_categorical_accuracy: 0.3451 - val_sparse_categorical_accuracy: 0.8671\n",
      "Epoch 17/25\n",
      "435759/435759 [==============================] - 21s 48us/sample - loss: 0.3606 - accuracy: 0.8503 - categorical_accuracy: 0.3439 - sparse_categorical_accuracy: 0.8503 - val_loss: 0.3196 - val_accuracy: 0.8701 - val_categorical_accuracy: 0.3601 - val_sparse_categorical_accuracy: 0.8701\n",
      "Epoch 18/25\n",
      "435759/435759 [==============================] - 21s 49us/sample - loss: 0.3591 - accuracy: 0.8511 - categorical_accuracy: 0.3442 - sparse_categorical_accuracy: 0.8511 - val_loss: 0.3188 - val_accuracy: 0.8726 - val_categorical_accuracy: 0.3566 - val_sparse_categorical_accuracy: 0.8726\n",
      "Epoch 19/25\n",
      "435759/435759 [==============================] - 25s 57us/sample - loss: 0.3559 - accuracy: 0.8527 - categorical_accuracy: 0.3446 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.3119 - val_accuracy: 0.8736 - val_categorical_accuracy: 0.3371 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 20/25\n",
      "435759/435759 [==============================] - 29s 66us/sample - loss: 0.3548 - accuracy: 0.8530 - categorical_accuracy: 0.3456 - sparse_categorical_accuracy: 0.8530 - val_loss: 0.3125 - val_accuracy: 0.8747 - val_categorical_accuracy: 0.3478 - val_sparse_categorical_accuracy: 0.8747\n",
      "Epoch 21/25\n",
      "435759/435759 [==============================] - 21s 49us/sample - loss: 0.3533 - accuracy: 0.8535 - categorical_accuracy: 0.3464 - sparse_categorical_accuracy: 0.8535 - val_loss: 0.3093 - val_accuracy: 0.8742 - val_categorical_accuracy: 0.3414 - val_sparse_categorical_accuracy: 0.8742\n",
      "Epoch 22/25\n",
      "435759/435759 [==============================] - 22s 50us/sample - loss: 0.3517 - accuracy: 0.8546 - categorical_accuracy: 0.3442 - sparse_categorical_accuracy: 0.8546 - val_loss: 0.3116 - val_accuracy: 0.8733 - val_categorical_accuracy: 0.3325 - val_sparse_categorical_accuracy: 0.8733\n",
      "Epoch 23/25\n",
      "435759/435759 [==============================] - 21s 49us/sample - loss: 0.3505 - accuracy: 0.8554 - categorical_accuracy: 0.3457 - sparse_categorical_accuracy: 0.8554 - val_loss: 0.3106 - val_accuracy: 0.8747 - val_categorical_accuracy: 0.3693 - val_sparse_categorical_accuracy: 0.8747\n",
      "Epoch 24/25\n",
      "435759/435759 [==============================] - 22s 50us/sample - loss: 0.3498 - accuracy: 0.8557 - categorical_accuracy: 0.3449 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.3074 - val_accuracy: 0.8755 - val_categorical_accuracy: 0.3294 - val_sparse_categorical_accuracy: 0.8755\n",
      "Epoch 25/25\n",
      "435759/435759 [==============================] - 20s 47us/sample - loss: 0.3482 - accuracy: 0.8560 - categorical_accuracy: 0.3445 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.3055 - val_accuracy: 0.8783 - val_categorical_accuracy: 0.3486 - val_sparse_categorical_accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6e690aded0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train.values, \n",
    "          y=y_train.values.ravel(), \n",
    "          epochs=25, \n",
    "          validation_data=(X_test.values, y_test.values.ravel()), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the DNN and training for 25 epochs we achieve an accuracy close 88% in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a random forest for comparison's sake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.8811119312939465\n",
      "\n",
      " Confusion Matrix: \n",
      " [[50198  2641     1     0     8     1   111]\n",
      " [ 2815 67651   165     1    85    88    20]\n",
      " [    1   204  8476    42     6   209     0]\n",
      " [    0     3   108   550     0    26     0]\n",
      " [   45   504    27     0  1788     9     0]\n",
      " [   11   184   508    26     3  3610     0]\n",
      " [  320    32     0     0     2     0  4774]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 10, \n",
    "                             criterion = 'entropy', \n",
    "                             random_state = 42,\n",
    "                             class_weight = 'balanced')\n",
    "\n",
    "rfc.fit(X_train, y_train.values.ravel())\n",
    "rfc_predictions = rfc.predict(X_test) \n",
    "\n",
    "bal_accuracy = balanced_accuracy_score(y_test, rfc_predictions)\n",
    "print(f'Balanced accuracy: {bal_accuracy}')\n",
    "\n",
    "cm = confusion_matrix(y_test, rfc_predictions) \n",
    "print(f'\\n Confusion Matrix: \\n {cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once again we achieved a balanced accuracy close to 88%. Both models presented the same accuracy but Random Forest did so in a fraction of the time.\n",
    "\n",
    "It seems that the heavy imbalance of classes is tampering with our predictive models. With this in mind synthetic data generation seems like a reasonable choice to further increase the accuracy of our models, as well as trying out other classification algorithms. However for the time being I will leave the notebook as it is and maybe I will extend it in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
